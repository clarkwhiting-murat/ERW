

## Input Data Types and Formats
The system ingests three distinct categories of input data, each serving a specific role in the modeling workflow. These inputs are ingested through separate API endpoints and stored in normalized database tables.

### Scenario Configuration Data
Scenario configuration data defines the material properties and deployment conditions for a specific analysis. Each scenario requires a unique configuration identifier and deposit identifier, both generated automatically from human-readable names provided by the user. The configuration includes the application rate measured in tonnes per hectare, which directly scales the absolute CO₂ removal potential. Particle size is recorded as a floating-point value in millimeters, typically representing the D50 or median particle size. Finer particles expose more surface area per unit mass, leading to faster initial weathering rates, though the relationship is not linear due to aggregation and surface passivation effects. Material chemistry is captured through two oxide percentages: magnesium oxide and calcium oxide, both measured as weight percentages from assay reports. These values are combined into a single chemistry factor K using the formula K = (MgO% × 1.09 + CaO% × 0.785) / 100. This factor represents the theoretical CO₂ storage capacity per tonne of rock material. The coefficients 1.09 and 0.785 reflect the stoichiometric relationships between these oxides and CO₂ uptake, accounting for the molecular weights and reaction pathways. The system calculates this factor automatically when users provide MgO and CaO percentages, avoiding manual stoichiometric calculations while maintaining consistency across scenarios. Climate setting is selected from a predefined set of categories: COLD_WET, TEMPERATE, or HOT_DRY. These categories represent broad climatic regimes that influence temperature and moisture patterns throughout the simulation period. The climate category affects weathering behavior at a high level but does not attempt to represent fine-scale inter-annual variability or monthly weather fluctuations. This simplification reflects the current design scope and available inputs, acknowledging that more granular climate data could improve predictions but would require additional data sources and computational complexity. Optional fields include a rock product name or blend code, which distinguishes different grinds or blends from the same deposit. This allows users to compare scenarios that differ only in particle size distribution or mixing strategies while maintaining traceability to the source deposit. The system stores all configuration data in the configurations table, linked to a deposit record through a foreign key relationship.
### Simulation Laboratory Testing Data
Simulation laboratory testing data consists of controlled experiments where rock material is exposed to CO₂ under well-defined conditions. These experiments are not intended to reproduce the full complexity of soil systems. Instead, they provide direct measurements of intrinsic weathering behavior under stable and repeatable exposure conditions. The data format requires two columns: time_months and co2_uptake_t_per_t. Time_months is a floating-point value representing the elapsed time since the start of the experiment, measured in months. Values can include fractional months, such as 1.5 representing six weeks or 2.25 representing nine weeks. The co2_uptake_t_per_t field records cumulative CO₂ uptake per tonne of rock material, measured in tonnes CO₂ per tonne of rock. This is a cumulative measurement, meaning that a value of 0.5 at month 1.0 indicates that half a tonne of CO₂ has been taken up per tonne of rock material over the first month. The system ingests this data through CSV file uploads, where users provide a file containing header rows and data rows. The frontend validates that required columns are present before parsing the CSV into JSON format. Each record is then sent to the backend API endpoint `/lab-data/ingest` as a JSON array. The backend validates that the associated configuration exists before storing records in the lab_records table. Records are indexed by configuration ID and time_months, allowing efficient retrieval ordered by time for the Extended Kalman Filter processing.Early time points in laboratory data indicate how quickly the material begins to react, while later time points reveal whether uptake curves are continuing to rise or beginning to flatten. These patterns inform the latent monthly reaction rate that the model uses in long-term projections. Laboratory conditions usually hold temperature and moisture constant within a narrow band, and they do not include field-scale factors such as soil structure or plant root dynamics. Still, they provide clean evidence that is not confounded by operational variability. Without simulation laboratory testing, the model would be forced to infer the weathering rate solely from field evidence, which is often sparser and more variable.
### Field Observation Data
Field observation data captures CO₂ removal achieved under actual deployment conditions over defined time windows. These measurements may be derived from soil carbon sampling, gas flux measurement, or other monitoring methods applied over specific periods, typically twelve-month windows. The data format requires three columns: window_start, window_end, and co2_removed_t_ha. Window_start and window_end are integer values representing the start and end months of the measurement window, measured from the deployment start date. A window_start of 0 and window_end of 12 represents the first year after deployment. The co2_removed_t_ha field records total CO₂ removed per hectare during that time window, measured in tonnes CO₂ per hectare. This differs from laboratory data in that it represents an aggregate measurement over a time period rather than a point-in-time cumulative measurement. Field data reflects the combined effects of rock properties, soil environment, microbial processes, climate variability, and management practices. The model uses field measurements as confirmation data, showing how the material performs in real conditions and adjusting the latent weathering rate estimated from simulation laboratory testing. In some cases, field results align closely with laboratory expectations. In other cases, they deviate due to site-specific factors such as drier conditions or different soil chemistry. When deviations occur, the model does not treat them as errors that must be discarded. They are used to update the state of the underlying variables so that future projections are anchored in what is observed in the field. Field data are often less regular than laboratory data. Sampling windows may change over time, and not all deployments will have the same intensity of monitoring. The Bayesian update step accounts for this by weighting each observation according to its uncertainty, though the current implementation uses equal weighting. Observations with clearer signals and better measurement quality would ideally have more influence on the updated state, but this requires explicit uncertainty quantification in the input data, which is not currently captured.
### Soil Laboratory Analysis Data
Soil laboratory analysis data consists of measurements on soil samples taken from deployment sites and processed in a laboratory to determine properties such as pH, cation exchange capacity, texture, mineralogical composition, and related indicators. This data is ingested through CSV files containing two columns: soil_property and value. The system validates that required columns are present and rejects duplicate soil_property entries to prevent conflicting measurements. Numeric properties such as pH, CEC, EC, and bulk_density are automatically converted to floating-point values during parsing. Text properties such as texture, mineralogy, and organic_matter are stored as strings. The system stores this data in memory for report generation but does not send it to the backend database. This reflects the current design where soil analysis serves as contextual evidence for interpretation rather than driving the core simulation model. Soil pH can influence the speciation of dissolved carbon, affecting whether CO₂ remains in solution or precipitates as carbonate minerals. Texture influences water holding capacity and gas diffusion rates, which affect both weathering rates and CO₂ transport. Soil laboratory analysis helps explain why a particular site exhibits higher or lower field-measured CO₂ removal than expected from simulation laboratory testing alone. In this version of the system, soil analysis is treated as contextual evidence stored alongside the scenario and field data, supporting interpretation and future model refinement rather than driving the core simulation itself.

## Simulation Workflow
The simulation workflow consists of three sequential steps: model state update, Monte Carlo simulation, and results summarization. Each step depends on the previous step's outputs, creating a pipeline where data flows from raw observations through statistical inference to final predictions.
### Model State Update
The model state update step uses an Extended Kalman Filter to fit model parameters to available evidence. This step begins by loading the configuration record, all associated laboratory records ordered by time_months, and all associated field records ordered by window_start. The system requires at least one laboratory record to proceed, as laboratory data provides the primary signal for estimating weathering rates. The Extended Kalman Filter operates on two latent variables: eta_r, representing the log-transformed weathering rate, and eta_phi, representing the logit-transformed retention factor. These transformations ensure that the variables remain within valid ranges: weathering rates must be positive, and retention factors must be between zero and one. The log transformation for weathering rates handles the wide range of possible values while maintaining numerical stability. The logit transformation for retention factors maps the unit interval to the real line, allowing the filter to operate in an unconstrained space. Initial priors are set using the first laboratory observation. The eta_r0_mean is initialized as the natural logarithm of the first co2_uptake_t_per_t value, with a floor of 1e-6 to prevent numerical issues. The eta_r0_var is set to 1.0, representing moderate initial uncertainty. If field records are available, eta_phi0_mean is initialized as the natural logarithm of the mean field removal rate. Otherwise, it defaults to 0.0, which corresponds to a retention factor of 0.5 after logit transformation. The Extended Kalman Filter processes laboratory observations sequentially, performing prediction and update steps for each time point. The prediction step advances the state according to the Markov transition model: eta_r[t] = eta_r[t-1] + alpha_r + Normal(0, sigma_r), where alpha_r represents the drift term and sigma_r represents the process noise standard deviation. The update step incorporates the observation using the Kalman gain, which balances the relative uncertainty in the prediction versus the observation. The observation model assumes y_lab ~ Normal(exp(eta_r), sigma_lab), where sigma_lab represents measurement error. The filter uses linearization around the current state estimate, computing the Jacobian of the observation function h(eta_r) = exp(eta_r) as H = exp(eta_r). This linearization is valid when the uncertainty is small relative to the state value, which is typically the case after a few observations. The innovation covariance S = H² × P_r_pred + sigma_lab² combines prediction uncertainty and measurement uncertainty. The Kalman gain K = (H × P_r_pred) / S determines how much weight to give the new observation versus the prior prediction. Parameter fitting uses maximum likelihood estimation with the L-BFGS-B optimization algorithm. The objective function is the negative log-likelihood computed by running the Extended Kalman Filter with candidate parameter values. Initial parameter guesses are alpha_r = 0.0, alpha_phi = 0.0, sigma_r = 0.1, sigma_phi = 0.1, and sigma_lab = 0.1 × std(y_lab). Bounds constrain alpha_r and alpha_phi to [-1.0, 1.0], while sigma parameters are constrained to [1e-6, 10.0] to ensure positive values and prevent numerical instability. The optimization typically converges within 1000 iterations, though most cases converge much faster. Upon convergence, the system runs a final Extended Kalman Filter pass with the fitted parameters to obtain final state estimates and posterior variances. These values are stored in the state_posteriors table, including eta_r0_mean, eta_r0_var, eta_phi0_mean, eta_phi0_var, alpha_r, alpha_phi, sigma_r, and sigma_phi. The posterior means and variances represent the updated beliefs about the latent variables after incorporating all available evidence.
### Monte Carlo Simulation
The Monte Carlo simulation step generates thousands of possible future trajectories by sampling from the posterior distributions and propagating them forward through time. The number of simulation runs is configurable, with typical values ranging from 100 to 10,000. More runs provide smoother percentile estimates but increase computational time linearly. Each simulation run begins by sampling initial values for eta_r0 and eta_phi0 from their posterior distributions. The system draws eta_r0 from Normal(eta_r0_mean, sqrt(eta_r0_var)) and eta_phi0 from Normal(eta_phi0_mean, sqrt(eta_phi0_var)). These initial values represent one plausible starting state consistent with the evidence. The simulation then advances month by month over a 120-month horizon, representing ten years of deployment. For each month t, the system applies the Markov transition model: eta_r[t] = eta_r[t-1] + alpha_r + Normal(0, sigma_r) and eta_phi[t] = eta_phi[t-1] + alpha_phi + Normal(0, sigma_phi). The Normal(0, sigma) terms represent process noise, capturing natural variability in weathering rates and retention factors that cannot be predicted deterministically. After each transition, the system transforms the latent variables to physical quantities: r_t = exp(eta_r[t]) and phi_t = expit(eta_phi[t]), where expit is the logistic function 1 / (1 + exp(-eta_phi)). The weathering rate r_t represents tonnes of CO₂ taken up per tonne of rock per month. The retention factor phi_t represents the fraction of reacted CO₂ that remains stored over the long term. Monthly CO₂ removal is calculated as N[t] = A × K × r[t] × phi[t], where A is the application rate in tonnes per hectare and K is the chemistry factor. This formula combines the material-specific chemistry factor with the time-varying weathering rate and retention factor, scaled by the application rate. The result N[t] represents tonnes of CO₂ removed per hectare in month t. If a risk profile is specified, the system applies risk multipliers to the monthly removal values. Risk multipliers can be provided as arrays, dictionaries, or single values. Array multipliers are applied month-by-month, with the last value used for months beyond the array length. Dictionary multipliers can specify month-specific values or a default value. Single-value multipliers apply uniformly across all months. These multipliers represent discrete risks such as drought, misapplication, or erosion, reducing CO₂ removal in affected months while maintaining the underlying geochemical signal. After simulating all 120 months, the system sums the monthly removals to compute n_total = sum(N[t] for t = 0 to 119). This total represents cumulative CO₂ removal over the ten-year period for that simulation run. The system stores each n_total value along with the run index in the simulation_results table, allowing later retrieval and analysis.
### Results Summarization
The results summarization step computes statistical summaries from the collection of n_total values generated by the Monte Carlo simulation. If no simulation results exist for a configuration, the system can trigger a new simulation run by calling the simulation service with specified parameters. Otherwise, it retrieves existing results from the simulation_results table. The system computes several summary statistics: mean, standard deviation, minimum, maximum, and percentiles at the 5th, 50th, and 95th positions. The mean represents the average outcome across all simulated futures, providing a measure of central expectation. The median or 50th percentile indicates the value in the middle of the distribution, where half of outcomes are above and half are below. The 5th percentile represents a conservative lower bound, where 95% of outcomes lie at or above this value. The 95th percentile represents an upper bound, where 95% of outcomes lie at or below this value. Percentile calculation uses NumPy's percentile function, which interpolates between adjacent values when the exact percentile position falls between data points. For the 5th percentile with 1000 simulation runs, this corresponds to approximately the 50th smallest value, though interpolation provides smoother estimates when the exact position is not an integer. The system also computes P_hit, the probability that total CO₂ removal meets or exceeds a specified target value. The target defaults to 10 times the application rate if not explicitly provided, representing a reasonable performance threshold. P_hit is calculated as the fraction of simulation runs where n_total >= target. This metric is useful for internal investment decisions, answering the question of how likely a scenario is to hit a required CO₂ performance threshold. All summary statistics are returned as floating-point values in a JSON response, along with metadata such as the number of simulation runs, the target value used, and the configuration ID. The frontend displays these values with appropriate formatting and explanatory text, helping users understand what each statistic represents and how to interpret them for regulatory crediting purposes.

## Evidence Integration and Bayesian Updating
The system integrates evidence through Bayesian updating, where prior beliefs about latent variables are revised in light of new observations. The Extended Kalman Filter implements this updating process sequentially, processing observations one at a time and maintaining probability distributions over the latent variables. Laboratory observations provide direct information about the weathering rate through the observation model y_lab ~ Normal(exp(eta_r), sigma_lab). Each observation narrows the posterior distribution for eta_r, reducing uncertainty about the weathering rate. The amount of narrowing depends on the relative magnitudes of prediction uncertainty and measurement uncertainty. When measurement uncertainty is small relative to prediction uncertainty, observations have substantial influence. When measurement uncertainty is large, observations have less influence, preventing noisy data from causing abrupt or unrealistic shifts. Field observations provide indirect information about both weathering rates and retention factors through aggregate measurements over time windows. The current implementation does not explicitly incorporate field observations into the Extended Kalman Filter update step. Instead, field data influences the initial prior for eta_phi0_mean if available. Future implementations could incorporate field observations more directly by modeling them as functions of both weathering rate and retention factor, though this requires additional modeling assumptions about how field measurements relate to the underlying latent variables. The Markov structure ensures that each month's state depends only on the previous month's state plus process noise, not on the full history of states. This prevents inappropriate reuse of historical data from other deposits or scenarios. Once an update has occurred, the model does not reopen past states, preserving forward consistency. This structure matches the continuously progressing nature of weathering behavior, where each month builds upon the previous month's state of the material. As more evidence accumulates, the posterior distributions become narrower, meaning uncertainty is reduced. When evidence is strong and precise, the updated distribution becomes much narrower than the prior. When evidence is weak or noisy, the updated distribution remains broader, acknowledging that more than one value of the latent variables is consistent with the data. This uncertainty propagation is crucial for the Monte Carlo simulation step, where wider distributions lead to greater variability in simulated outcomes.

## Calculations and Numerical Methods
The system performs several types of calculations: statistical inference through the Extended Kalman Filter, optimization through maximum likelihood estimation, stochastic simulation through Monte Carlo methods, and statistical summarization through percentile calculations. The Extended Kalman Filter calculations involve matrix operations on covariance matrices, though the current implementation uses scalar operations since the latent variables are treated as independent. The prediction step computes P_r_pred = P_r + sigma_r², where P_r is the current variance and sigma_r² is the process noise variance. The update step computes P_r = (1 - K × H) × P_r_pred, where K is the Kalman gain and H is the observation Jacobian. These calculations maintain numerical stability by ensuring variances remain positive and bounded. The log-likelihood calculation accumulates contributions from each observation: log_likelihood += -0.5 × (log(2π × S) + (innovation²) / S), where S is the innovation covariance and innovation is the difference between observed and predicted values. The negative log-likelihood is minimized during parameter fitting, with the L-BFGS-B algorithm handling bound constraints and approximating the Hessian matrix for efficient convergence. Monte Carlo simulation uses NumPy's random number generation functions, specifically np.random.normal for sampling from Gaussian distributions. Each simulation run uses independent random draws, ensuring that runs are statistically independent. The system seeds the random number generator implicitly through NumPy's default behavior, though explicit seeding could improve reproducibility for debugging purposes. Percentile calculations use NumPy's percentile function with linear interpolation. For a sorted array of n values, the p-th percentile corresponds to the value at position (p/100) × (n-1) in the sorted array, with interpolation between adjacent values when the position is not an integer. This provides smooth percentile estimates that are less sensitive to the exact number of simulation runs than methods that use only integer positions. The chemistry factor calculation K = (MgO% × 1.09 + CaO% × 0.785) / 100 combines the oxide percentages with stoichiometric coefficients. The coefficient 1.09 represents the ratio of CO₂ molecular weight to MgO molecular weight times the stoichiometric factor for the reaction MgO + CO₂ → MgCO₃. Similarly, 0.785 represents the corresponding ratio for CaO. These coefficients assume complete reaction of the oxides, which is an upper bound on actual performance. The division by 100 converts percentages to fractions.

## Report Generation
The system generates comprehensive narrative reports that document the entire analysis workflow, from scenario definition through evidence integration to final predictions. Reports are generated client-side in JavaScript and can be downloaded as plain text files for regulatory submission or archival purposes. Report generation begins by assembling the full regulatory narrative, which provides a plain-language explanation of the modeling approach, assumptions, and boundary conditions. This narrative is stored as a constant in the frontend code and appears at the beginning of every report, ensuring consistency across analyses. The mathematical model explanation section describes the Bayesian updating process, Markov state transition structure, and Monte Carlo simulation approach. This section explains how laboratory and field data jointly update latent variables, how uncertainty shrinks as evidence accumulates, and how thousands of simulated trajectories cover the full range of possible futures. The explanation progresses from basic concepts to more technical details, accommodating readers with varying levels of statistical background. The scenario input summary section lists all configuration parameters: scenario name, deposit name, rock product name, particle size, application rate, MgO and CaO percentages, calculated chemistry factor, and climate setting. This provides traceability and allows reviewers to verify that inputs match their expectations. The simulation laboratory testing data summary section lists all laboratory records with their time_months and co2_uptake_t_per_t values. This allows reviewers to assess the quality and quantity of laboratory evidence and understand how it informs the model. The soil laboratory analysis summary section lists all soil properties and their measured values, providing context for interpreting field observations. The field observation data summary section lists all field records with their time windows and CO₂ removal measurements. This shows how field data confirms or adjusts laboratory-based predictions and provides real-world validation of the model's projections. The simulation results interpretation section presents the statistical summaries: mean, median, 5th percentile, and 95th percentile values, all measured in tonnes CO₂ per hectare over the ten-year period. The section explains what each statistic represents and how to interpret them. The 5th percentile receives particular attention as the conservative value intended for carbon credit issuance, with explanation of why this conservative approach protects against over-crediting. The report includes a target performance assessment if a target value was specified, showing the probability that total CO₂ removal meets or exceeds that target. This metric helps users assess whether a scenario is likely to meet performance requirements for investment or regulatory purposes. The closing statement confirms that the report contains all scenario inputs, evidence, and modeled outputs used to produce the 10-year CO₂ retention forecast. This provides a clear audit trail for regulatory review and ensures that all relevant information is documented in a single location. Reports are generated on-demand when users click the "Generate Report Text" button. The system assembles the report from stored data in the reportData object, which accumulates information as users progress through the workflow. If any required data is missing, the report includes placeholder text indicating what information is unavailable. Users can download the report as a plain text file with a filename that includes the scenario name and timestamp, facilitating version control and archival.

